{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "We will be using a collection of images of dogs, sorted by breed, as our dataset for this lab.  The dataset can be found at [footnote 1](#footnote1); it was created by Stanford graduate students and their advisor as a data set specifically for fine-grained image categorization. Besides the categorized pictures of dogs, the data set also includes bounding box annotations for each of the images, class labels, and information on how the creators used the set to train and test.  The full set contains 20,560 images, with 120 different breeds of dogs represented.  There are at least 148 images of each breed, with the Maltese dog having the most images at 252.  Each picture is at least 200 pixels by 200 pixels in size, with most pictures generally not being much larger than that.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This data is a specifically created set of images, created using images found on ImageNet under the 'Canis familiaris' node.  The three graduate students, Khosla, Jayadevaprakash, and Yao, along with their mentor, Fei-Fei, created this set of images specifically with the purpose of creating a dataset for fine-grained image categorization.  They went through the images found on ImageNet one breed at a time, being sure to select images that were only classified under one node, and removed images which had distortion, major occlusion, and other defects, in order to create a subset of images that they believed addressed the drawbacks of other fine-grained image categorization datasets.  Further information about their process and their results in achieving their dataset goals can be found at [footnote 2](#footnote2)\n",
    "\n",
    "### Prediction Task\n",
    "\n",
    "The prediction task for this dataset is relatively simple: to be able to recognize the breed of a dog from a picture, which may be of different age, color, shape than the original images, or which may be partially occluded. Generally, there is little inter-class variation in the appearance of dogs <sup>[[1]](#footnote1)</sup> and large intra-class variation<sup>[[1]](#footnote1)</sup>, making recognizing dogs a tough challenge for categorization.  Even with a human, there are certain types of dogs that pose challenges when it comes to differentiation, especially among mixed breed dogs; to be able to identify the type of dog in an image mechanically would help in cases as simple as a person trying to identify what breeds are in a mongrel's ancestry. <br>\n",
    "At a more advanced level, it would be helpful to many photo sharing websites to be able to recognize a type of dog from the image posted, in order to create more tailored advertising for said person.  To be able to recognize a dog is to be able to advertise generalized dog products at a user, while to be able to recognize breed is to be able to target a user for very specific dog-related purchases, which may drive higher advertising revenue.  It has been demonstrated how much more effective targeted ads can be than their non-targeted counterparts<sup>[[3]](#footnote3)</sup>; to continue to truly personalize ads based on the very specifics of a person's lifestyle is the ultimate goal of web advertisment. Advertising is what drives the modern web, and the modern dog owner spends \\$1549<sup>[[4]](#footnote4)</sup> per year on dog-related purchases, to be able to further target advertising has the potential to be very valuable.\n",
    "\n",
    "### Third Parties\n",
    "\n",
    "The third parties that would be interested in such a classification system would be a combination of social media sites and advertisers.  Working in unison, they would be able to look at the pictures posted on the social media account of a user, and not only determine what kind of pet they have, they could go further and determine the subtype of that pet, in order to maximize personalization of the ads.  The most obvious example of a site that would use a system such as this one would be Instagram. Instagram is entirely based around lifestyle and images, and pet photos are very common. In the past, Instagram has been cited as a model that may have a difficult time monetizing, so to be able to further increase the probability of someone clicking on an advertisment is very valuable.  Similar rationales exist with Facebook, Twitter, even Snapchat (albeit somewhat ethically fuzzier with Snap).  Overall, any site that relies on user-generated content and advertising would likely reap the benefits of being able to classify pets as granularly as possible.\n",
    "\n",
    "### Cited\n",
    "<a name=\"footnote1\">1</a>: http://vision.stanford.edu/aditya86/ImageNetDogs/ <br>\n",
    "<a name=\"footnote2\">2</a>: http://people.csail.mit.edu/khosla/papers/fgvc2011.pdf <br>\n",
    "<a name=\"footnote3\">3</a>: https://hbr.org/2016/04/targeted-ads-dont-just-make-you-more-likely-to-buy-they-can-change-how-you-think-about-yourself <br>\n",
    "<a name=\"footnote4\">4</a>: http://www.americanpetproducts.org/press_industrytrends.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "#Get rid of memory cached by jupyter notebook\n",
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import data, color\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "\n",
    "\n",
    "#Set the image size and path to the data\n",
    "img_size = (300,400)\n",
    "img_path = 'Images/'\n",
    "img_paths = []\n",
    "img_annotations = []\n",
    "\n",
    "#The images are stored in subdirectories - go through all of them\n",
    "for dirpath, dirnames, filenames in os.walk(img_path):\n",
    "    for filename in [f for f in filenames if f.endswith(\".jpg\")]:\n",
    "        img_paths.append(os.path.join(dirpath, filename))\n",
    "        img_annotations.append(dirpath.split('/')[1].split('-')[1])\n",
    "\n",
    "print(img_paths[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_data = []\n",
    "for path in img_paths[0:1000]:\n",
    "    img_data.append(color.rgb2grey(resize(plt.imread(path).astype(np.uint8),img_size)).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4665d45e3fe7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    plt.imshow(img_data[random.randint(0,len(img_data))].reshape(img_size),cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skimage.feature import corner_harris, corner_subpix, corner_peaks\n",
    "from skimage.transform import warp, AffineTransform\n",
    "\n",
    "\n",
    "tform = AffineTransform(scale=(1.3, 1.1), rotation=1, shear=0.7,\n",
    "                        translation=(210, 50))\n",
    "image = warp(data.checkerboard(), tform.inverse, output_shape=(350, 350))\n",
    "image = img_data[random.randint(0,len(img_data))].reshape(img_size)\n",
    "coords = corner_peaks(corner_harris(image), min_distance=5)\n",
    "coords_subpix = corner_subpix(image, coords, window_size=13)\n",
    "\n",
    "plt.gray()\n",
    "plt.imshow(image, interpolation='nearest')\n",
    "plt.plot(coords_subpix[:, 1], coords_subpix[:, 0], '+r', markersize=15, mew=5)\n",
    "plt.plot(coords[:, 1], coords[:, 0], '.b', markersize=7)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
